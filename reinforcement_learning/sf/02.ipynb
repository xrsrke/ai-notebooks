{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e76bcec-4079-429e-9f7d-73a886483d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046875e-740b-4ebf-9552-1fd1ff7915fd",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e3b6233-8df8-4c5e-a487-c2ccb8a445b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = torch.tensor([\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.4, 0.1, 0.5],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f148ac80-e8c3-4cde-8af3-9945bd180b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db69098-7654-44fe-84c1-dcd93c7f3a1c",
   "metadata": {},
   "source": [
    "`transition_matrix` is the Markov chain transition matrix is defined with three states `(s1, s2, s3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bfb7bdd-557b-4644-966b-3374e1ad8d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.2000, 0.3000],\n",
       "        [0.1000, 0.7000, 0.2000],\n",
       "        [0.4000, 0.1000, 0.5000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad7f18-72e3-4c3d-9d29-30895b33f819",
   "metadata": {},
   "source": [
    "What is the probability from state `s3` to state `s2`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bad41499-9087-4dca-8e6a-87e031af8fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix[2][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06964ae-7a72-4c62-9ee2-a5f1f8aa7fba",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2618c04b-0c2f-43b2-b88f-5f19c8db6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72610bc-116d-49d5-86f3-52317d83a960",
   "metadata": {},
   "source": [
    "`transition_matrix` is the Markov chain transition matrix is defined with three states `(s1, s2, s3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5803bb49-2754-4c23-add1-09094b0ea53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.2000, 0.3000],\n",
       "        [0.1000, 0.7000, 0.2000],\n",
       "        [0.4000, 0.1000, 0.5000]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03fcd8-1902-44dd-b3d5-300595a92cfc",
   "metadata": {},
   "source": [
    "Extract the probability of next state given the current state is `s3` using matrix multiplication. And interpret the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "132f5071-7163-4715-9c3b-6e43db328dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = torch.tensor([0, 0, 1]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a64b8db5-ec0a-4ce3-814c-f5f27246abdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc2e00e5-dc77-4f52-9f4e-a505809a5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_each_next_state = current_state @ transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c20f465b-e24b-4972-a3bc-2a44a943d611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4000, 0.1000, 0.5000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_each_next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58275475-33b5-4122-9f89-f77fa230cdb4",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "\n",
    "The probability move from state `s3` to state:\n",
    "- `s1` is `0.4000`\n",
    "- `s2` is `0.1000`\n",
    "- `s3` (stay where it's) is `0.5000`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d9e294-6720-4ce1-a8c1-65ef98752420",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6ac2d25-ed9b-4f7c-9e1d-d1b09eedc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = [[10, 20, 30],\n",
    "                 [40, 50, 60],\n",
    "                 [70, 80, 90]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f043ed56-d643-4831-bcc7-03bdb7ba127e",
   "metadata": {},
   "source": [
    "`reward_matrix` is reward matrix for actions `(act1, act2, act3)` in states `(s1, s2, s3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93770607-85f8-4b64-860d-bd09a163b0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 20, 30], [40, 50, 60], [70, 80, 90]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558f4e1-803e-4556-bf19-85704067ee63",
   "metadata": {},
   "source": [
    "What is the reward if agent is in state `s2` takes action `act3`? Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51705e0e-23f2-4e3c-bd4d-0a3b62c9f996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_matrix[1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6028e4-123e-4b0e-9462-e618c58baa19",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "\n",
    "- **Step 1**: To extract all the rewards in state `s2` => `reward[1]`\n",
    "- **Step 2**: From there, action `act3` has index `2` => `reward[1][2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e1796-72c8-409e-9cba-5f4b46e9fb5b",
   "metadata": {},
   "source": [
    "##### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df0b5af-cba3-47cf-9ee7-530420d6f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = torch.tensor([[10, 20, 30],\n",
    "                              [40, 50, 60],\n",
    "                              [70, 80, 90]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa48dfd2-f02e-40fd-a585-67020757672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.tensor([10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de48b84d-40f3-4f78-ac9a-c28db14259e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = torch.tensor(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "667dab94-aaa7-4389-9adb-4ee16af287fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1534672-af68-4dae-a2bd-d764159a8977",
   "metadata": {},
   "source": [
    "Given `rewards` is a reward vector of the sequence of next actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81a86c2f-e6a7-4448-8274-02e95eaff3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6903aff3-b832-4fa2-9d81-2cba922feefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f491bc-2c2a-4908-851c-244b77133d48",
   "metadata": {},
   "source": [
    "Write the `calculate_return` that calculate the return as bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e1e9dd6-c903-4a84-9374-9779c1cac52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return(rewards, discount_factor):\n",
    "    total_return = torch.zeros(1)\n",
    "    \n",
    "    for i in range(len(rewards)):\n",
    "        discounted_reward = rewards[i] * (discount_factor ** i)\n",
    "        total_return += discounted_reward\n",
    "        \n",
    "    return total_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a734f315-b64a-447b-9c38-3a1eab89ff04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([114.2650])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_return(rewards, discount_factor=discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85c6ac-a550-4e94-82ab-bb149cd412bb",
   "metadata": {},
   "source": [
    "##### Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73508ef-ad8c-472b-80c3-732407c3232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "actions = [\"Left\", \"Right\"]\n",
    "rewards = {\n",
    "    \"A\": {\"Left\": 10, \"Right\": 20},\n",
    "    \"B\": {\"Left\": 5, \"Right\": 15},\n",
    "    \"C\": {\"Left\": 20, \"Right\": 10},\n",
    "    \"D\": {\"Left\": -10, \"Right\": -20},\n",
    "    \"E\": {\"Left\": -5, \"Right\": -15},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca4ba1-bba9-40ff-9f3e-566005759858",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423dc81e-9621-43c0-8cae-6d546edbb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value of each state in the MRP\n",
    "values = {}\n",
    "for state in states:\n",
    "    value = 0\n",
    "    for action in actions:\n",
    "        value += rewards[state][action] * gamma\n",
    "    values[state] = value\n",
    "\n",
    "# Print the values of each state in the MRP\n",
    "for state in states:\n",
    "    print(f\"Value of state {state}: {values[state]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
