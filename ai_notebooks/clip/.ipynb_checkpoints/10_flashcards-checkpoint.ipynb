{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd51864-e12a-45af-898e-513f92c9ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RN50', 'openai'), ('RN50', 'yfcc15m'), ('RN50', 'cc12m'), ('RN50-quickgelu', 'openai'), ('RN50-quickgelu', 'yfcc15m'), ('RN50-quickgelu', 'cc12m'), ('RN101', 'openai'), ('RN101', 'yfcc15m'), ('RN101-quickgelu', 'openai'), ('RN101-quickgelu', 'yfcc15m'), ('RN50x4', 'openai'), ('RN50x16', 'openai'), ('RN50x64', 'openai'), ('ViT-B-32', 'openai'), ('ViT-B-32', 'laion400m_e31'), ('ViT-B-32', 'laion400m_e32'), ('ViT-B-32', 'laion2b_e16'), ('ViT-B-32', 'laion2b_s34b_b79k'), ('ViT-B-32-quickgelu', 'openai'), ('ViT-B-32-quickgelu', 'laion400m_e31'), ('ViT-B-32-quickgelu', 'laion400m_e32'), ('ViT-B-16', 'openai'), ('ViT-B-16', 'laion400m_e31'), ('ViT-B-16', 'laion400m_e32'), ('ViT-B-16-plus-240', 'laion400m_e31'), ('ViT-B-16-plus-240', 'laion400m_e32'), ('ViT-L-14', 'openai'), ('ViT-L-14', 'laion400m_e31'), ('ViT-L-14', 'laion400m_e32'), ('ViT-L-14', 'laion2b_s32b_b82k'), ('ViT-L-14-336', 'openai'), ('ViT-H-14', 'laion2b_s32b_b79k'), ('ViT-g-14', 'laion2b_s12b_b42k'), ('roberta-ViT-B-32', 'laion2b_s12b_b32k'), ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'), ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from tglcourse.utils import *\n",
    "\n",
    "print(open_clip.list_pretrained())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e129fc45-69ac-4eb6-8618-31cae8879896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 605M/605M [00:07<00:00, 82.1MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f7515-4aa8-4bec-8d2a-87b86e51b5da",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc04c9-651a-474b-a1fb-7271b0c7bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = pil_from_url('https://images.pexels.com/photos/185032/pexels-photo-185032.jpeg?').resize((600, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be380d19-7eaf-45ca-920f-3f285d8b2ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = preprocess(input_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0c9d5d79-3d11-4c8c-b683-4de9b90b075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"persistent is all you need\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "24221b97-a298-4360-bcb8-e2a0ab6238c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0981b-4328-4d89-aff0-a3ffa428ca89",
   "metadata": {},
   "source": [
    "`image_batch` is a batch of 1 color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3b54e6a8-2186-4501-982f-4523a82cc7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "24704320-989b-4df6-861e-b06d1c058039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'persistent is all you need'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caf378bb-ac17-40ca-b2e0-d8ae190fee02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open_clip.model.CLIP"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b414908-63d0-49b9-ac0c-11ca13fa542b",
   "metadata": {},
   "source": [
    "Use OpenCLIP to encodes the `image_batch` and `sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1be140a-a13b-45da-9b12-b248ffcab652",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = open_clip.tokenize([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c76aa09-015a-4ec8-a9bd-18aa743b1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_batch)\n",
    "    text_features = model.encode_text(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cf60d2d-b45e-41c2-beec-57111ab7afc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape, text_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7bb037-1dc0-4b01-972b-bf404879c082",
   "metadata": {},
   "source": [
    "##### Example 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f767ae3d-2daa-4241-8190-cb09fe418b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = preprocess(input_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5c14e57e-a1f7-42f0-a5a2-aca408696014",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"persistent is all you need\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5923c943-dac2-4316-8ed0-9511c01ccef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = open_clip.tokenize([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "02ba7921-5524-480c-a9de-2b3b724f384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab24cac-752e-4d21-944e-c5dad7c3bb60",
   "metadata": {},
   "source": [
    "`image_batch` is a batch of 1 color image, `text_batch` is a batch of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "340236c7-3dfc-485a-96fe-ae86d9f2fc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 224, 224]), torch.Size([1, 3, 224, 224]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_batch.shape, image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "58fc150c-b841-4641-8a1a-c80522f9df62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open_clip.model.CLIP"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0c5ab-d79b-484c-ae39-a281ff5917fb",
   "metadata": {},
   "source": [
    "Use OpenCLIP to encodes the `image_batch` and `sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6c85ba7c-f92c-4b5a-af39-a0b0a620aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_batch)\n",
    "    text_features = model.encode_text(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "687135f3-d3c8-4af9-af82-3b6244e35ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape, text_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de43257b-17ad-4be7-9eb0-f5d3ceabb53e",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaa715a1-3904-4a76-8ba9-d3c1bf8ab863",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a cat\", \"spider man walks on the beach\", \"starship on mar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299ea961-2b8c-4ebf-a8c1-4bf69f1999b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = open_clip.tokenize(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b35eeaa-2edc-4345-8df3-b770df4e15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = model.encode_text(text_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f390df-444e-4e8e-b7b0-15a934d0e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = pil_from_url('https://images.pexels.com/photos/185032/pexels-photo-185032.jpeg?').resize((600, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd47b0e-3d6c-4f13-8fbe-bb99d8bf0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch = preprocess(input_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "728bac1b-e75a-47c4-8740-02187e17376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = model.encode_image(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51e04f25-9349-4274-9aef-0b8b15271418",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = text_embedding.detach()\n",
    "image_embedding = image_embedding.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5d835e5-ef30-4f09-9eb8-c76df9f91dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c792c-e016-4452-ab26-5ee48644c49f",
   "metadata": {},
   "source": [
    "`image_embedding` is the embedding of a cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c85e5b5-80c0-49b7-9f20-fe01bd876c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa402f6b-aabc-4666-8138-20606ad142b5",
   "metadata": {},
   "source": [
    "`text_embedding` is a batch of embedding of three sentences\n",
    "- a cat\n",
    "- spider man walks on the beach\n",
    "- starship on mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "68eef263-9353-4efc-bfc5-4124c82fa184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa751599-9842-410c-a094-154f2d7d5acc",
   "metadata": {},
   "source": [
    "Write a function calculate the probabily that each sentence in `text_embedding` has the meaning same as the `image_embedding`\n",
    "\n",
    "**Hint**: Normalize the embedding first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ebb7530-f66e-486b-b5d2-06a0b729b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_scores(image_embedding, text_embedding):\n",
    "    image_embedding_norm = image_embedding.norm(dim=-1, keepdim=True)\n",
    "    image_embedding = image_embedding / image_embedding_norm\n",
    "    \n",
    "    text_embedding_norm = text_features.norm(dim=-1, keepdim=True)\n",
    "    text_embedding = text_embedding / text_embedding_norm\n",
    "    \n",
    "    similarities = image_embedding @ text_embedding.T\n",
    "    probs = F.softmax(similarities, dim=-1)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8f514a3f-b658-4747-8282-fa0a4f94bddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3555, 0.3055, 0.3390]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_scores(image_embedding, text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e01e0-bcba-49a0-a277-36dbe3197803",
   "metadata": {},
   "source": [
    "### Projection Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849ca83-7d48-48de-9ea8-04fc56363df6",
   "metadata": {},
   "source": [
    "##### Example 1: GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78dfd529-820e-4893-9296-039cd984a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75ec7fdd-3799-4b66-890e-d0d4756089cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu = nn.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b62da857-072e-48fc-98f1-d093fc756d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-2, -1, 0, 1, 2]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a68b7304-94a8-4b15-9a6b-f51a743345c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ea00f62-4ce0-4c48-981d-320022f353ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5183)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc620c1-3706-4115-93a2-26bdc2027c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8030)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98b4a4a5-0c6d-4e35-b31d-97406c035c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0.,    0.,    0.,  122., 2099.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gelu(torch.tensor([-20, -1000, 0, 122, 2099]).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c51821-169b-4da3-b62d-50aeea52910a",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf8ee2-77f9-459b-bc78-70fc9fe22a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90f4b805-4bed-425c-9654-d8c0c73bea4c",
   "metadata": {},
   "source": [
    "### Contrastive loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a342918-35a4-4202-8f53-d7b1030968cc",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2876a89-9326-4c7f-abce-ba2c49ca3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9b8cd5e-bb17-4e93-986a-4a969d8a0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.randn(10, 128)\n",
    "image_embeddings = torch.randn(10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "198c614e-d745-4a9b-8744-2e86410b8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "792e3798-86d4-48b5-988d-dbaff6f88cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2498854-7e20-4100-a925-7a5fcd885670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c163f96f-7df6-4b52-8101-b88c34060f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 128]), torch.Size([10, 128]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape, image_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa7f016-0516-4298-a601-ebeef9aee1a9",
   "metadata": {},
   "source": [
    "Given `text_embeddings` is the embedding of text and `image_embeddings` is the embedding of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdca8f-182b-4206-9bf3-088279babbb2",
   "metadata": {},
   "source": [
    "Calculate the target distribution for both images and texts in CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f8d5571-4dc2-4421-9d0a-d2ae42eb8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_similarity = image_embeddings @ image_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f5ffa4a-c661-48c5-b40b-26f09d766952",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_similarity = text_embeddings @ text_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1430ad8-4eaf-46e6-80ad-f0477a124c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = F.softmax(\n",
    "    (images_similarity + texts_similarity) / 2 * temperature,\n",
    "    dim=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d1e93b4-9ff7-4f50-97ca-2e5d565567b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964ab29-0f3b-456d-ab29-2a33bbdc42e9",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a57a45f-32eb-4e96-b491-e889a9c410f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = (text_embeddings @ image_embeddings.T) / temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6444b7-6a5c-4959-8689-9a0b289b517a",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b4497a68-95b0-4d20-b209-273b167fad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dim = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d3bace5e-1aae-4bdc-8baf-58ced84fa930",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.randn(batch_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c09be743-4c54-43f7-89e1-2887e9f3f3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = embeddings @ embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3fa43d59-995f-4db8-bab2-6867743ce95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[139.4657,  -8.0346,   8.9255,  -1.7976],\n",
       "        [ -8.0346, 134.4699, -10.2236,  19.8583],\n",
       "        [  8.9255, -10.2236, 106.3404,  14.3997],\n",
       "        [ -1.7976,  19.8583,  14.3997, 137.2661]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1304ea50-3a07-4e9a-ac28-1bd112eb04a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.9326e-43, 0.0000e+00, 1.0000e+00, 1.1767e-40],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(F.softmax(out, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61742ba0-ff28-4482-8043-709fa1c3710f",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8fa87-9f8d-42ba-95dd-687f797584cd",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86a204-8512-45c8-9d1a-f2003dcb05b8",
   "metadata": {},
   "source": [
    "Write a pseudocode for training CLIP. Given\n",
    "- `images` is a list of all tensor images\n",
    "- `texts` is a list of all texts\n",
    "\n",
    "**Hint:** No need batch training, just do a simple loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10fd3b-9de5-4a76-8170-ab9d07404805",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in n_epochs:\n",
    "    image_features = text_encoder(text)\n",
    "    text_features = text_encoder(text)\n",
    "    \n",
    "    image_embeddings = image_projection(image_features)\n",
    "    text_embeddings = text_projection(text_features)\n",
    "    \n",
    "    logits = (text_embeddings @ image_embeddings.T) / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2428484-0bda-4076-8500-e63213fffeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
