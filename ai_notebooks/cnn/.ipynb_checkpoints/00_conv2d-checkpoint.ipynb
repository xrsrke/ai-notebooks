{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412c5246-879d-47f9-b38a-5c524bd207f2",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab563240-5594-4de2-b191-910845295b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df11f0-b203-44d1-9cab-a3d73350c227",
   "metadata": {},
   "source": [
    "Create a random tensor that represents a batch of 50 colors images, each have size `64x64`\n",
    "\n",
    "Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d78c8b32-d2a9-4529-84eb-190f09d2c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images = torch.randn(50, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59132a70-f9de-40f2-a137-5132a4a18bba",
   "metadata": {},
   "source": [
    "**Explain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e156c63-78a2-4519-8683-88fffcab7b21",
   "metadata": {},
   "source": [
    "`(minibatch, in_channels, iH, iW)` => `(50, 3, 64, 64)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78d058-ffe8-4c6d-af14-3ac38d5c6217",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cf025d5-5cfc-4a09-bc2f-50373319aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4734a17e-9441-4752-b07f-be01a4c22ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b34824e-102c-4e97-b019-b0d457a107a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e03c63-9285-49ee-9b8a-cd4a116df42e",
   "metadata": {},
   "source": [
    "Suppose `x` is a batch of 32 colour images, each of size 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e9f62a1-8062-4b52-bb79-b833166acb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 128, 128])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "597f4ba3-4d30-4499-9044-8151f4d86939",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=3,  # number of channels in the input (lower layer)\n",
    "                  out_channels=7, # number of channels in the output (next layer)\n",
    "                  kernel_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21d1ea-b2d6-4890-ab87-bb653654bc83",
   "metadata": {},
   "source": [
    "What is the shape of the batch image after the `conv1`? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5499844-d5ab-405f-ab86-20e5f0c0b2b0",
   "metadata": {},
   "source": [
    "Create a `conv2d` that output 7 filterred color images for each image in the batch, and using kernel size `5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df5f5210-6c89-48ee-931c-b5b35445e79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 7, 124, 124])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e14872-01c5-4862-b579-47973e309a27",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382fb580-e149-4d13-a01c-455cb12cedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77743a40-3ffc-4859-be88-50d0637cc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = torch.rand(3, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7cb94-6bdd-4e76-8eb8-41c6bab0a12e",
   "metadata": {},
   "source": [
    "Suppose `im` is a color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9cbd8af-0e26-4f71-a668-b25f46804484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417cfae5-7df0-40c4-8e31-abbdb9373eb5",
   "metadata": {},
   "source": [
    "Create a random kernel size `2x2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f56e2814-e087-4182-8e92-3a5268f06669",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = torch.rand(3, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0b5fce-acaa-435c-b3a8-b0eb9784f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871b5d5-b2c3-4214-8546-69a8a598c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Conv2d(in_channels=3, out_channels=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f9080-8184-495a-b393-879f0cbd0819",
   "metadata": {},
   "source": [
    "##### Example 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76415fb-9cb2-403a-8663-30b68504cbcd",
   "metadata": {},
   "source": [
    "- `input_shape`: a tuple contains `depth x width x high` of the input\n",
    "- `kernel_size`: the size of each matrix, inside each kernel\n",
    "- `depth`: number of kernel (=> depth of the output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658393b4-2ab1-4fbf-993b-7fb2c535d174",
   "metadata": {},
   "source": [
    "`self.kernel_shape`: stores the shape of all kernels, each kernel is a 3D array\n",
    "- `depth`: the number of kernels\n",
    "- `input_depth`: the depth of the input\n",
    "- `kernel_size`: the size of matrixes inside each kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd5f555b-8a71-4fe0-8049-86f84d1e61eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Convolution(nn.Module):\n",
    "    def __init__(self, input_shape, kernel_size, n_kernels):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.n_kernels = n_kernels\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        \n",
    "        self.output_shape = (n_kernels,\n",
    "                             input_height - kernel_size + 1,\n",
    "                             input_width - kernel_size + 1)\n",
    "        self.kernel_shape = (n_kernels, input_depth, kernel_size, kernel_size)\n",
    "        \n",
    "        self.kernels = torch.rand(*self.kernel_shape)\n",
    "        self.biases = torch.rand(*self.output_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = torch.clone(self.biases)\n",
    "        \n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.depth):\n",
    "                # self.output[i] += \n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8fea432-a01e-40c9-b341-70ef5f4cf97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd408a25-27c7-414c-ba65-116d8201b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = np.array([3, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f619ee54-3d44-49c1-9877-50c5f650fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = np.array([2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29978646-af6c-46ce-9900-7396023469a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.array([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9588937f-889e-4e29-8fe6-40db589f5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = Convolution(input_shape, kernel_size, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac2403a5-f805-4963-ad1d-083e99e2dfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5]), array([63, 63]), array([63, 63]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0135dbc-9823-4d29-89b1-86457ca01238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
