{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3885db-b682-49b7-9813-1c72fb94fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde4a0f8-67d8-45ff-acf8-6617fbdb1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d689a4-3a5d-47ec-b121-34ed0c8c9792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(nn.Module):\n",
    "    def __init__(self, input_shape, kernel_size, n_kernels):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        kernel_height, kernel_width = kernel_size\n",
    "        \n",
    "        self.n_kernels = n_kernels\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        \n",
    "        self.output_shape = (n_kernels,\n",
    "                             input_height - kernel_height + 1,\n",
    "                             input_width - kernel_width + 1)\n",
    "        self.kernel_shape = (n_kernels, input_depth, kernel_height, kernel_width)\n",
    "        \n",
    "        self.kernels = torch.rand(*self.kernel_shape)\n",
    "        self.biases = torch.rand(*self.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd05efe-1f19-4ba7-a91e-f6e5b7c6c664",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca66f209-f7b0-46df-b057-398e058cdfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Convolution(input_shape=(3, 28, 28), kernel_size=(4, 4), n_kernels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543761de-b88f-4e20-82a8-cfd9d73a758d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3, 4, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.kernel_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775096c-1c95-499d-8873-9b8034f70116",
   "metadata": {},
   "source": [
    "What's shape of the kernel (one matrix contains all the kernels)? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076f77b-5e72-4ea6-a188-2a1c9d17ae58",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "- `7`: the number of kernels, equals to `n_kernels = 7`\n",
    "- `3`: the depth of the image (aka: number of channels), equals to `input_shape[0] = 3`\n",
    "- `4`: the `kernel_height` (aka: number of rows) of each invidiual kernel, equals to `kernel_size[0] = 4`\n",
    "- `4`: the `kernel_width` (aka: number of columns) of each invidiual kernel, equals to `kernel_size[1] = 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fbd7db-3241-4d4c-a19c-2d0b852a37b3",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d2b78f-dc99-40e2-bdf3-8b3e529ef93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Convolution(input_shape=(3, 28, 28), kernel_size=(2, 2), n_kernels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad2e328-0501-4c7d-b1f6-fd2729b18eff",
   "metadata": {},
   "source": [
    "What is the output of the forward pass? Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244bd0ea-0642-4f2d-9032-a0ac0f4c98de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 27, 27)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a48b0-4ceb-46cc-83fe-8eb76404880c",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "- `5`: the number of kernels, equals to `n_kernels = 5`\n",
    "- `27`: the height of the output (aka rows), computed by `input_height - kernel_height + 1 = 28 - 2 + 1 = 27`\n",
    "- `27`: the width of the output (aka columns), computed by `input_width - kernel_width + 1 = 28 - 2 + 1 = 27`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac0e1e-226c-4a75-ac75-5b7a0ccf7a94",
   "metadata": {},
   "source": [
    "##### Example 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d315d4a-1abc-48a8-a110-f968c909ed8c",
   "metadata": {},
   "source": [
    "`nn.Conv2d` contains 3 parameters: `in_channels`, `out_channels` and `kernel_size`\n",
    "\n",
    "Explain the meaning of each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d2370-e661-4584-a83a-a51cf68d91e8",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "- `in_channels`: the depth of the image (aka: color channels)\n",
    "- `out_channel`: the number of kernels\n",
    "- `kernel_size`: r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762cc4d-857e-4fdd-8b0e-a7527b598db5",
   "metadata": {},
   "source": [
    "##### Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e221a49d-30a9-4e9d-bb4f-ef3070d6743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv2d(in_channels=3, out_channels=7, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5775b7-5ca0-4973-8140-190fe17ae421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc3127-63b4-41ee-a6c4-70746167b99d",
   "metadata": {},
   "source": [
    "##### Example 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ee3588f3-dd96-4893-ac8d-c24ffd37fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "_image = torch.arange(12288).reshape(3, 64, 64).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "097fa05a-790e-405d-9d26-da2ba88411d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7b16a4e3-3e79-45a1-8c3a-fa52e5861130",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "75c90878-e0ca-49c4-81cb-84f9396ef179",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2dfbf644-aeb8-4f77-aa34-d699ea227a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee4e4b-417b-400c-ba75-4498e3ef4dcc",
   "metadata": {},
   "source": [
    "What is the output after each layer? Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f80fb01e-fc6c-45e9-b1b1-74dbda6c025e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "38f68aa5-78ff-43a9-a97a-3fc96a6bd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv1(_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "583c2d98-bb2e-493d-9409-5106da5a421b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 60, 60])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57e8fd-53e2-4646-b16e-8140eedfc268",
   "metadata": {},
   "source": [
    "- `16` = `out_channel`\n",
    "- `60` = `output_height` = `input_height - kernel_height + 1` = `64-5+1`\n",
    "- `60` = `output_width` = `input_width - kernel_width + 1` = `64-5+1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c1f7ca89-4ffa-4350-9c25-032c6df2496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pool(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "afeabbe4-bdbf-4459-98b4-0918a65c1f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 30, 30])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5ed829a4-a396-4062-9ff6-190c90c59bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc98f4ee-8924-4f7c-abc4-22add009bb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 26, 26])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3506f8a-30da-40f2-8504-0e1ee2140b6c",
   "metadata": {},
   "source": [
    "- `16` = `out_channel`\n",
    "- `26` = `output_height` = `input_height - kernel_height + 1` = `26=30-5+1`\n",
    "- `26` = `output_width` = `width_height - kernel_width + 1` = `26=30-5+1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61181f20-f2da-4c12-8931-410af449a739",
   "metadata": {},
   "source": [
    "##### Example 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ccd773ab-9b1b-4fcd-9379-f9b437396824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shape_after_max_pool(input_width, kernel_size, padding, stride):\n",
    "    return ((input_width - kernel_size + 2*padding) / stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a32dfdbf-de25-4d14-b41b-8adb6d9fc6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_shape_after_max_pool(input_width=60, kernel_size=2, padding=0, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d3af6699-5142-49fb-a970-f1c8a7d4c1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_shape_after_max_pool(5, 3, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06f5a6-3c4e-4008-9551-6185c686f421",
   "metadata": {},
   "source": [
    "##### Example 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "deadef6b-6808-49d9-b1ef-3c7aa5552ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_image = torch.arange(3*64*64).reshape(3, 64, 64).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f0a8c0c5-8c30-4654-a4fc-0fd3b3ad3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f35236c2-fc84-4362-b7b5-ce18a63410ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2d6ee-b830-402e-ba28-ee74331270fd",
   "metadata": {},
   "source": [
    "Apply a convolution to `_image` with kernel size is `3x3`. `_image` is a color image with size `64x64`\n",
    "\n",
    "Explain how you choose the parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2a8b8806-adcc-4590-bfae-ab4312708a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cc60a-a47d-4247-b513-0914afcf27a6",
   "metadata": {},
   "source": [
    "**Explain**\n",
    "\n",
    "So the image output has shape `5x64x64`\n",
    "\n",
    "- `in_channels` = `3` because the depth of image (aka: number of color channels) is `3`\n",
    "- `out_channels` = `5` because the output image has five kernels\n",
    "- With `stride=1` and `padding=1`, it will keep the height and width of the output image as the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1f932737-9540-4beb-8a93-dfe45afc27d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64, 64])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(_image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65131fa7-5c7d-4444-b20f-ce65a21eebc8",
   "metadata": {},
   "source": [
    "##### Example 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3e81a41-0f3b-4107-8857-199fa0a25fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.rand(4, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd8f2f19-6cc1-4783-be03-3d04e815bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724983e-7b06-474f-8069-870424f5cb83",
   "metadata": {},
   "source": [
    "`batch` is a batch of 4 colors image, each imaga have size `32x32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce2db732-7c8f-46a4-aad6-90b29140fc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8ce3b-4713-4a73-998d-41923afa297b",
   "metadata": {},
   "source": [
    "Flatten the pixels of each color channel using `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a06dbd00-0051-429b-940e-0c148219cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten(start_dim=2, end_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8650e9d9-dca3-4498-bfb2-fc1fc6b9d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_batch = flatten(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8e10e93-d49a-4420-90f7-3727d3aa9b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 1024])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d99541-b1d6-47cb-a678-c03b8b103b4e",
   "metadata": {},
   "source": [
    "##### Example 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d06d660-4100-4dc5-b610-60b27c437d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.rand(4, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35b8678c-bc61-4550-a891-ffc045f351e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299f57e-b491-47bf-be95-6b1b81072801",
   "metadata": {},
   "source": [
    "`batch` is a batch of 4 colors image, each imaga have size `32x32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "05a4ca85-72d9-4c7a-83f4-200e5ad99043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f4bec-9470-42cd-bd75-6d578d4adab7",
   "metadata": {},
   "source": [
    "Flatten the three colors channel of each image using `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d22473e1-9e21-4574-bf2e-1572f901b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten(start_dim=1, end_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d35a5d6a-b58f-43de-a265-7c690a26aa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_batch = flatten(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8d71767-e993-40aa-9c70-16485b7e29d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3072])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e52bee-0d4c-4093-b933-5b51594304b1",
   "metadata": {},
   "source": [
    "##### Example 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddef8706-89c5-4e65-81cf-d2fdd9dfebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_image = torch.arange(3*64*64).reshape(3, 64, 64).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e0ab16c-7b5a-4e0a-9cec-a4ade7fa41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f18ca04c-cd6e-4d8b-ba3c-55466cac88bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841f01e-45af-472d-b7cb-b0289efe836a",
   "metadata": {},
   "source": [
    "Apply a convolution to `_image` with kernel size is `3x3`. `_image` is a color image with size `64x64`\n",
    "\n",
    "Explain how you choose the parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78486885-4d30-4779-aff5-d16a30f7aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "731b4be1-f3ce-4127-97cb-460ed4f555c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 62, 62])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56d5d9-df73-426e-aaae-c3095921b6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
