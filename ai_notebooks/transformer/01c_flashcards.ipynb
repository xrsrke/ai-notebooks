{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023a7135-cf31-4021-b4cb-dade383ad177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73661677-d407-47fc-ab18-ac0ba3a25008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819dda0f-1a25-473d-bd05-203c5d7625e7",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46711717-cc84-41d3-aa90-4ed462735ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Persistent is all you need\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f211c-76ce-47b5-af5b-ce065d89718f",
   "metadata": {},
   "source": [
    "Now we create a function the return the correspond index of each word in an sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7628f6c-b115-4e28-a936-be44539db935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2idx(x):\n",
    "    words = x.split(' ')\n",
    "    return [i for i, w in enumerate(words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c6497-75fa-4667-aa99-014359a17da4",
   "metadata": {},
   "source": [
    "Here is the formula for calculate the positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13bdcf-26d5-4ec8-bf51-b1f146defcab",
   "metadata": {},
   "source": [
    "$\\text{Positional Vector}(p, i_{odd})=\\sin \\left(\\frac{\\mathrm{p}}{\\mathrm{n}^{2 \\mathrm{i} / \\mathrm{d}}}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748eac06-04dc-4349-9b06-2e3cfe5c8cc2",
   "metadata": {},
   "source": [
    "$\\text{Positional Vector}(p, i_{even})=\\cos \\left(\\frac{\\mathrm{p}}{\\mathrm{n}^{2 \\mathrm{i} / \\mathrm{d}}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c27797a4-787f-4e7e-bb55-82d470d2055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEncoding:\n",
    "    def __init__(self, n, d_model):\n",
    "        self.d = d\n",
    "        self.n = n\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def encode(self, idxs):\n",
    "        n_words = len(idxs)\n",
    "        self.embeddings = np.zeros((n_words, self.d_model))\n",
    "        \n",
    "        for p in range(n_words):\n",
    "            for i in range(self.d):\n",
    "                self.embeddings[p][i] = self.by_sin(p, i, self.d) if i % 2 == 0 else self.by_cosin(p, i, self.d_model)\n",
    "        \n",
    "        return self.embeddings\n",
    "    \n",
    "    def by_sin(self, p, i, d):\n",
    "        denomiator = np.power(self.n, (2*i/d))\n",
    "        return np.sin(p / denomiator)\n",
    "    \n",
    "    def by_cosin(self, p, i, d):\n",
    "        denomiator = np.power(self.n, (2*i/d))\n",
    "        return np.cos(p / denomiator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37281e93-b112-4cb4-9059-71142f9bde26",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPostionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'd'"
     ]
    }
   ],
   "source": [
    "encoder = PostionalEncoding(n=200, d=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af539f9b-79a8-4887-8edf-78c932d2e011",
   "metadata": {},
   "source": [
    "`text` is the text need to encode, and `idxs` is the list of indexes corresponds to each word in `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0992f2ef-56b2-4f69-9953-d7b35727d5e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idxs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text, \u001b[43midxs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'idxs' is not defined"
     ]
    }
   ],
   "source": [
    "text, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d46a6ad-28d5-45ac-b3d6-d921a256d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = encoder.encode(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e43ed18-acb9-48dd-8632-13f8890d2c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 5),\n",
       " array([[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          1.00000000e+00,  0.00000000e+00],\n",
       "        [ 8.41470985e-01,  9.92795169e-01,  1.44264986e-02,\n",
       "          9.99998499e-01,  2.08138300e-04],\n",
       "        [ 9.09297427e-01,  9.71284494e-01,  2.88499945e-02,\n",
       "          9.99993994e-01,  4.16276592e-04],\n",
       "        [ 1.41120008e-01,  9.35777938e-01,  4.32674858e-02,\n",
       "          9.99986487e-01,  6.24414865e-04],\n",
       "        [-7.56802495e-01,  8.86787137e-01,  5.76759716e-02,\n",
       "          9.99975978e-01,  8.32553111e-04]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7764f8-b280-4ae2-b51f-732e4f61f6d0",
   "metadata": {},
   "source": [
    "##### Example 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd3aa66-b9cf-42f0-b54a-e6e22303cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Persistent is all you need\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c9dc1-15c9-4a7c-ab59-0ccd43e579ab",
   "metadata": {},
   "source": [
    "Now we create a function the return the correspond index of each word in an sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46748246-63d5-42db-9392-a08384e7a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2idx(x):\n",
    "    words = x.split(' ')\n",
    "    return [i for i, w in enumerate(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ee6039-fb97-4764-a0d1-96caff9a6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = text2idx(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed3358b2-727e-446d-819c-c6ba36a846f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e053ad-a4d5-457f-87ab-1bc5016a0220",
   "metadata": {},
   "source": [
    "Here is the formula for calculate the positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb250f-3ce1-4119-96c0-606e08f91d13",
   "metadata": {},
   "source": [
    "$\\text{Positional Vector}(p, i_{odd})=\\sin \\left(\\frac{\\mathrm{p}}{\\mathrm{n}^{2 \\mathrm{i} / \\mathrm{d}}}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db676bc-82d1-4808-9ad1-40bbf37a75b0",
   "metadata": {},
   "source": [
    "$\\text{Positional Vector}(p, i_{even})=\\cos \\left(\\frac{\\mathrm{p}}{\\mathrm{n}^{2 \\mathrm{i} / \\mathrm{d}}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc08355-169c-45a9-938c-c7396dcfbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, n, d_model):\n",
    "        self.n = n\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def encode(self, idxs):\n",
    "        n_words = len(idxs)\n",
    "        embeddings = np.zeros((n_words, self.d_model))\n",
    "        \n",
    "        for p in range(n_words):\n",
    "            for i in range(self.d_head):\n",
    "                denominator = np.power(self.n, (2*i/self.d_model))\n",
    "                embeddings[p][i] = np.sin(p / denominator) if i % 2 == 0 else np.cos(p / denominator)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a9ee47-e0e0-4183-bc1f-d8df8f3d5111",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'd_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'd_head'"
     ]
    }
   ],
   "source": [
    "encoder = PositionalEncoding(n=200, d_head=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd90bcf-7466-45c4-b1ef-08c78092d698",
   "metadata": {},
   "source": [
    "`text` is the text need to encode, and `idxs` is the list of indexes corresponds to each word in `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a92471-59c6-41e7-8870-15ede4048f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce38ce32-e593-4b7f-9e38-28fc7f500377",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241m.\u001b[39mencode(idxs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings = encoder.encode(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9f1d364-d087-4adf-88d6-2fc89c3c9a2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membeddings\u001b[49m\u001b[38;5;241m.\u001b[39mshape, embeddings\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings.shape, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad0de8-36ec-45c2-9a55-1b1b29158c18",
   "metadata": {},
   "source": [
    "##### Example 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6811010c-f009-4f2a-a9e6-a9ed14566d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Persistent is all you need\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5ef2c-9f62-425b-a596-7d759f6a96b6",
   "metadata": {},
   "source": [
    "Now we create a function the return the correspond index of each word in an sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464c6048-5e70-4578-8d4b-a788efc1efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2idx(x):\n",
    "    words = x.split(' ')\n",
    "    return [i for i, w in enumerate(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16b09ebc-fd90-4551-a672-77a52835f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = text2idx(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12666580-440e-494c-9595-5a0821457b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149e29a-cff5-4fe4-b4b7-a9ba83fdf597",
   "metadata": {},
   "source": [
    "Here is the formula for calculate the positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cfb5a-b217-49df-8f2a-6dae5a6d5a70",
   "metadata": {},
   "source": [
    "$\\text{Positional Vector}(p, i_{odd})=\\sin \\left(\\frac{\\mathrm{p}}{\\mathrm{n}^{2 \\mathrm{i} / \\mathrm{d}}}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb6fa0-bc12-488f-a194-771c8a7f2bda",
   "metadata": {},
   "source": [
    "$\\text{Positional Vector}(p, i_{even})=\\cos \\left(\\frac{\\mathrm{p}}{\\mathrm{n}^{2 \\mathrm{i} / \\mathrm{d}}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d24f0a-a034-4932-8d18-3146fc90b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n, d_model):\n",
    "        self.n = n\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        n_words = len(tokens)\n",
    "        embeddings = np.zeros((n_words, self.d_model))\n",
    "        \n",
    "        for p in range(n_words):\n",
    "            for i in range(self.d_head):\n",
    "                denominator = np.power(self.n, (2*i/self.d_model))\n",
    "                embeddings[p][i] = torch.sin(p / denominator) if i % 2 == 0 else torch.cos(p / denominator)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4883c61-c01e-4a7f-a72f-e0b05be52e58",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'd_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'd_head'"
     ]
    }
   ],
   "source": [
    "encoder = PositionalEncoding(n=200, d_head=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9cb3a-bce9-473f-9c0c-fc248dd9a514",
   "metadata": {},
   "source": [
    "`text` is the text need to encode, and `idxs` is the list of indexes corresponds to each word in `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4434eb-e7f6-4ae8-9614-733ac4aafd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c19b03b-87c0-498b-bc24-81657d0257ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241m.\u001b[39mencode(idxs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings = encoder.encode(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3f7163-306e-41e5-a997-c9fbced57b7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43membeddings\u001b[49m\u001b[38;5;241m.\u001b[39mshape, embeddings\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings.shape, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05369d-76a6-4b0a-86b4-24bfcf165b07",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5582300f-6976-49d6-aacc-1f90a3ef2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[3, 4, 5], [1, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b51c9ee-61b2-407f-9e62-33ece88069de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d19960-827d-4e39-8f98-1de95a54cd9a",
   "metadata": {},
   "source": [
    "`x` has batch size `5`, each row contains a sequence of three words (or each column represent a word in a sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ad0bed9-0c5c-427b-bec5-29287dac8377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 5],\n",
       "        [1, 2, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "217ec235-16d3-4211-81ff-1ddf199e8d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c5471-17c9-41e6-b95e-d3b5766226b8",
   "metadata": {},
   "source": [
    "Create an embedding using `nn.Embedding`. And explain the `output` and each parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a97f9b5a-0669-4966-bb86-2046672ca3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=5, padding_idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77191003-c64c-448a-a45d-03d65a1e3f9a",
   "metadata": {},
   "source": [
    "**Explain**:\n",
    "- `num_embeddings` (can ignore): the number of words in vocabulary\n",
    "- `embedding_dim`: For each word, it will be represent by a vector of dim-5 (aka: a vector contain 5 numbers)\n",
    "- `padding_idx`: means any words with the value 0 in the input tensor will be represented by a vector of zeros\n",
    "    + In this case, value `5` in the first sequence, word 3th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11a46e61-ca3e-47db-94a9-822e2737c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99cc7d4b-27a6-41fe-86ad-7fd2a7cf3a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2679,  1.4846,  0.7749, -0.2949,  1.3814],\n",
       "         [-0.0090,  0.1373, -1.4240,  0.3321,  1.8965],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.8106, -0.2551,  0.7035,  1.2165, -0.4600],\n",
       "         [-0.4127, -0.5778, -1.2423, -0.3248,  1.3000],\n",
       "         [-0.1880,  1.2551,  0.7633,  1.7439, -0.1250]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bf82f01-1c82-4f2a-8d10-ad0b8e92a59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c586311b-79e7-4c2e-be59-7d085d0efcbd",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c79a5fe-f114-4164-b84d-4ef0a10a84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b36ac88-26d0-44f9-9cbe-1f9851027e28",
   "metadata": {},
   "source": [
    "Create a function that create a upper triangular matrix as bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a565d01-1639-49c6-8f8f-4da258bdb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(size):\n",
    "    return torch.zeros((size, size)).triu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6bc30db-ea56-4d55-b95c-7fda7634405d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_mask(size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c61a7-e634-4275-8659-b9316eb697e7",
   "metadata": {},
   "source": [
    "##### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05a6e87e-9f59-49bb-ae23-5b27ec5d606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ca072fd-b210-403b-be96-e40fc1979c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(size):\n",
    "    mask = torch.ones((1, size, size)).triu(1)\n",
    "    mask = mask == 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd0f9b69-da70-4b02-a222-fa09c3beb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = create_mask(size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c849058c-cd07-4e1b-8ff2-4accac26385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = torch.arange(16).reshape(1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ad663788-4e4e-43bc-80f8-158a07687fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2028f704-10fd-48ec-bcd2-3d3458adeb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac7feb-ccfc-4128-acd2-68350ed3e070",
   "metadata": {},
   "source": [
    "Given `mask` and `encoding`. Mask the `encoding` as bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79f9d789-14ee-4109-a0e4-27ca0ddadb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_encoding = encodings.masked_fill(mask == False, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa473bfc-a8ac-4d6a-a1cb-5817b2b92dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, -1, -1, -1],\n",
       "         [ 4,  5, -1, -1],\n",
       "         [ 8,  9, 10, -1],\n",
       "         [12, 13, 14, 15]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd9640-6f2c-4550-a39a-f81d6a51a248",
   "metadata": {},
   "source": [
    "##### Example 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ef8e0d-4fcf-4605-9847-5008f1250bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.tensor([[7, 8, 9], [10, 11, 12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d762f802-b857-4511-85b9-a88de11e53f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [4, 5, 6]]),\n",
       " tensor([[ 7,  8,  9],\n",
       "         [10, 11, 12]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a909e1-a34e-48b9-9aa7-cac20b1c9ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 16, 27],\n",
       "        [40, 55, 72]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ffb5a7b-a7c4-4ffc-8080-ca8c3ba608c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.einsum('ij,ij->ij', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d3d5aea-a549-40de-8dc9-3156b556dbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 16, 27],\n",
       "        [40, 55, 72]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70f4e2-f5b4-458f-8204-715dc32a3a4c",
   "metadata": {},
   "source": [
    "##### Example 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab6f558-c1a4-4391-9427-26a057cc6ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.tensor([[7, 8], [9, 10], [11, 12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a72d16c-5229-4be6-b410-ec935373357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [4, 5, 6]]),\n",
       " tensor([[ 7,  8],\n",
       "         [ 9, 10],\n",
       "         [11, 12]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97ae1b80-41c0-41c9-b408-95b5a8e85b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.einsum('ij,jk->ik', a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe7b16-84bc-4533-84c8-c7623af67c58",
   "metadata": {},
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74e509-0957-44c3-a3f7-b0af426f097e",
   "metadata": {},
   "source": [
    "##### Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee0f2cd-fc8f-4f50-b6f4-bd173bb5f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = torch.LongTensor([[1, 2, 3, 4, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8764a67-b2e8-438b-8072-8a551f786c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c100087-173d-4ab1-876a-f9dea17fd776",
   "metadata": {},
   "source": [
    "Write the word embedding layer in Transformer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e7d27-2f18-4756-8482-fcdc16c2f59d",
   "metadata": {},
   "source": [
    "**Hint**: multiply the embedding with the square root of the dimension of embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f64d3197-fe21-4c0a-9c4f-9f5596c5a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, padding_idx, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, d_model,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedding = self.embed(x)\n",
    "        return embedding * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "779a4081-8d7a-4763-b900-925fa4afd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = TextEmbedding(vocab_size=10, padding_idx=0, d_model=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfa37b-4747-4085-83a7-56a9403580d6",
   "metadata": {},
   "source": [
    "Calculate the embedding vector of `tokenized_input`, given you have 1000 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfcdc021-fbd8-453c-9500-25e026bd95ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 0, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b638752d-13a5-4bb7-87b6-a5f55c1f75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = text_embedding(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95db6c1c-8a26-4e38-86cd-aa76013b0d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3153e91-c17d-491d-b235-030caeca6c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5439,  0.3577,  1.7285, -1.0708,  3.8159],\n",
       "         [-3.3047, -1.6115,  1.9330,  4.6261, -0.9726],\n",
       "         [-0.9988,  1.0506,  1.2931,  1.7541, -0.5736],\n",
       "         [-0.8683,  2.3871, -2.4311, -1.1679, -3.5074],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c139b-d125-4166-a670-f98131c0dc3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f0582-3d50-4afe-a455-330c37b4a1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
