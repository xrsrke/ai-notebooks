{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a12cad9-6d2c-4543-84c5-5a78bfa672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f3a3c-0d46-4bb2-87b4-0923d52f6fb7",
   "metadata": {},
   "source": [
    "### 2. Creating a Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9e323cb-b1d4-422f-9604-f6c7b2b2fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERTbaseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "    \n",
    "    def compute_accuracy(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_size(self):\n",
    "        pass\n",
    "    \n",
    "    def time_pipline(self):\n",
    "        pass\n",
    "    \n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8492ccd8-a9c2-4a8c-9cb1-5ec2cd8cf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4378433-faa5-4bd6-8c08-8d0eb85ef4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset clinc_oos (/root/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a841d1559d44420bb9e90c2a819a60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clinc = load_dataset(\"clinc_oos\", \"plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c0d569a-e787-44e3-a150-83a3feba769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d59a1fc2-fff3-4da8-b5b3-3f8d71bcf736",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41ac6e79-a9da-42f7-aa87-8bc795ad1463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(self):\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    for example in self.dataset:\n",
    "        pred = self.pipeline(example[\"text\"][0][\"label\"])\n",
    "        label = example[\"intent\"]\n",
    "        preds.append(intents.str2int(pred))\n",
    "        labels.append(label)\n",
    "    \n",
    "    accuracy = accuracy_score.compute(\n",
    "        predictions=preds,\n",
    "        references=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abc634a1-fc23-4cf2-a506-60b2d1f0c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformanceBenchmark.compute_accuracy = compute_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fe721-3f78-4576-9bbb-c5076451da44",
   "metadata": {},
   "source": [
    "# Flashcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0652dd-9edf-437a-82d4-0520deee3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c5ac205-06b4-46b9-87dd-dac45664bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred = torch.tensor([[0, 1, 2], [3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a8513ab-7b1c-4819-9b9f-8abca73368dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_targ = torch.tensor([[0, 1, 2], [3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa3c520-981f-47f2-adb4-dbb2f5f376b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _targ = torch.tensor([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5515c40b-af16-4b2f-8e96-6e1777f0d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_metric.add_batch(predictions=_pred, references=_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3128a7c-ea91-44b9-9617-f1283d5baa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #_metric.compute(\n",
    "#     predictions=_pred,\n",
    "#     references=_targ\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da72a40-abec-499d-bae0-0986b197b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_metric.compute(\n",
    "    predictions=_pred,\n",
    "    references=_pred\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00e4d4-7371-4bbf-93ab-3d705033ae71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
