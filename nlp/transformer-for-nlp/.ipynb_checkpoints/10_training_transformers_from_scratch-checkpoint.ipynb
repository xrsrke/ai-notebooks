{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d336ed14-633e-40e5-be1e-723cb4eb6c8a",
   "metadata": {},
   "source": [
    "# 1. Large Datasets and Where to Find Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d3b9d0-c7fa-416c-aa64-2378338ee9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5518db2-721e-4109-a592-e82fc3466d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4779ba-3a7c-4155-a9a6-774e88389b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5dd263-87c8-420d-8656-9341202938fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code = r\"\"\"def say_hello():\n",
    "      print(\"Hello, World!\")\n",
    "# Print it\n",
    "say_hello()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec8d203f-bf13-4b8b-a4f5-e874848e67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92065ec3-0dc5-4f15-ac0b-514ee0476377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġsay',\n",
       " '_',\n",
       " 'hello',\n",
       " '():',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġprint',\n",
       " '(\"',\n",
       " 'Hello',\n",
       " ',',\n",
       " 'ĠWorld',\n",
       " '!\"',\n",
       " ')',\n",
       " 'Ċ',\n",
       " '#',\n",
       " 'ĠPrint',\n",
       " 'Ġit',\n",
       " 'Ċ',\n",
       " 'say',\n",
       " '_',\n",
       " 'hello',\n",
       " '()',\n",
       " 'Ċ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(python_code).tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cb471-5248-44a0-be44-d32b537212b3",
   "metadata": {},
   "source": [
    "# 2. Building a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1885a5b9-5926-456f-a69d-88b4d2dfd1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, e = u\"a\", u\"€\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889612f0-6a77-42e6-91b6-a2dd81715c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte = ord(a.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278e8a8c-247c-47b1-8bc8-aa64ba0deada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f80cc5a-7534-49a9-9c06-5514f8d547f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6b239f-16d1-4bee-ab3a-7d47b914824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[226, 130, 172]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19fe7bf0-cd1f-462d-bee6-2c9d8da9cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1322f22-2c1b-4e7a-a8b9-730c9a1d0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_to_unicode_map = bytes_to_unicode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b2f325-af5e-472d-8683-628c8b6ffd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e6f779-f03e-4e01-9994-40213d31cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unicode_to_byte_mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a6f9dcb-1269-43f3-86da-2d61b0f38ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vocab = list(unicode_to_byte_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff81d134-62b3-4511-be03-19e9826ec5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01866432-69df-4971-8df5-e0ec0f86fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f1cbd88-9657-43eb-a862-833a4ab08cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'!'}, {'\"'}, {'#'}, {'$'}, {'%'}, {'&'}, {\"'\"}, {'('}, {')'}, {'*'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{tokenizer.convert_tokens_to_string(t)} for t, _ in tokens[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c94b190f-d911-46c4-afd1-eafd09963cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd24ac-41b1-40fc-9ee6-18a111d4e9a4",
   "metadata": {},
   "source": [
    "### new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658017f1-b4b3-4db4-9f7f-0cf1f2c479f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc31a2b-b29c-4a8e-aa61-9102c2173d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d0a78a-1442-419d-9c8c-4845d5d134bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ebd0abf-b4cc-4ad2-9c8a-fbe48aa1ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"transformersbook/codeparrot-train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15941b5b-2f31-44fe-9980-62da37abe04a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[43mdataset_name\u001b[49m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_name' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a9f745-e099-49c0-8628-0740898da0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dataset = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f442763-c4d8-45fa-b251-cefe4babc7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56de0ac5-633a-49be-9b54-7d73ec37a15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch_iterator at 0x7f0075b79350>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf770d18-da4c-4d19-8b03-ede7f867df11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3d8b8253274100ab6856acbc95d0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(),\n",
    "    vocab_size=12500,\n",
    "    initial_alphabet=base_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b6939-33a9-47af-9b86-1ea9c610e7cc",
   "metadata": {},
   "source": [
    "### Training a Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5aad67-4cf4-4c7d-a51b-87b73b8f8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91c52e2-29ec-441a-82c9-6088f842f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"gpt2-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e80636-732a-47d9-9b9e-5133757dc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633d7c92-3068-4bf0-8035-5857b5a1bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size = len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44eb45a0-c9f6-434d-82b0-596ed337a1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2-xl\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 1600,\n",
       "  \"n_head\": 25,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 48,\n",
       "  \"n_positions\": 1024,\n",
       "  \"output_past\": true,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5d0438-7d5a-48d7-8690-6bcc2d26605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143e3507-665b-4bef-a9a6-d0c5185a6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, total_characters, total_tokens = 500, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788573f9-8552-4366-84a6-7c0f3970e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration transformersbook--codeparrot-train-39fd2cee2b2cb397\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"transformersbook/codeparrot-train\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a95359-76bc-42b7-bec6-cb7e5fb4de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:00<02:20,  3.54it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1310 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 500/500 [00:06<00:00, 77.02it/s] \n"
     ]
    }
   ],
   "source": [
    "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
    "    total_characters += len(example[\"content\"])\n",
    "    total_tokens += len(tokenizer(example[\"content\"]).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad3d5749-5e89-415e-a207-d703f3eeaeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5568371"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71c26e90-9c55-42bb-9e56-879aaddec48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2527740"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f22d5e0a-7b42-40fb-af76-01dd57af9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361bc9a-ac3c-40de-8d8a-5d8bf4dcf496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantLengthDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self, tokenizer, dataset, seq_length=1024,\n",
    "        num_of_sequences=1024, chars_per_token=3.6\n",
    "    ):\n",
    "        self.tokenizer = tokenizerenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * char_per_token * num_of_sequences\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        \n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            \n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m = f\"Buffer full: {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "102e6300-9bd4-46db-b35f-9d812e06d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantLengthDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, dataset, seq_length=1024,\n",
    "                 num_of_sequences=1024, chars_per_token=3.6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    break\n",
    "                try:\n",
    "                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    buffer.append(next(iterator)[\"content\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    iterator = iter(self.dataset)\n",
    "\n",
    "            all_token_ids = []\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
    "            for tokenized_input in tokenized_inputs['input_ids']:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            \n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb2751b-6870-4dba-800f-c4910abe0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.shuffle(buffer_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf7219-3cf8-4be2-ae66-a8009ed3976c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
